{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdHh9h01EM5i",
    "outputId": "244e0773-64b0-4aa5-eff0-0903bf19d6f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\bhavana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\bhavana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##importing necessary libraries and the train data\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import  time\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown\n",
    "brown_tagged =brown.tagged_sents( tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words and tags for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tredQIX2E3Sy"
   },
   "outputs": [],
   "source": [
    "brown_tagged_words=[]\n",
    "brown_tagged_tags=[]\n",
    "for sentence in brown_tagged:\n",
    "    word_sentence=[]\n",
    "    tag_sentence=[]\n",
    "    for words in sentence:\n",
    "        word_sentence.append(words[0])\n",
    "        tag_sentence.append(words[1])\n",
    "    brown_tagged_words.append(word_sentence)\n",
    "    brown_tagged_tags.append(tag_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cMUnYFMXIW9Q"
   },
   "outputs": [],
   "source": [
    "##encoding the words and tags to unique numbers because nn will work with numbers\n",
    "'''det =5\n",
    "   noun=1\n",
    "   adv = 7\n",
    "   verb =2\n",
    "   adp=4\n",
    "   adj=6\n",
    "   .=3\n",
    "   conj=9\n",
    "   pron=8\n",
    "   prt=10\n",
    "   num=11\n",
    "   x=12'''\n",
    "word_tokenizer = Tokenizer()                      \n",
    "word_tokenizer.fit_on_texts(brown_tagged_words)                    \n",
    "brown_tagged_words_encoded = word_tokenizer.texts_to_sequences(brown_tagged_words)\n",
    "\n",
    "tag_tokenizer = Tokenizer()                      \n",
    "tag_tokenizer.fit_on_texts(brown_tagged_tags)                    \n",
    "brown_tagged_tags_encoded = tag_tokenizer.texts_to_sequences(brown_tagged_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={5:\"DET\",1:\"NOUN\",7:\"ADV\" ,2:\"VERB\",4:\"ADP\",6:\"ADJ\",3:\".\",9:\"CONJ\",8:\"PRON\" ,10:\"PRT\",11:\"NUM\",12:\"X\"}\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "14D0-zz-Jd2R",
    "outputId": "9df97d49-f7ef-4bb9-ad17-70aa90397d4a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence: 180\n"
     ]
    }
   ],
   "source": [
    "## we will pad now\n",
    "lengths = [len(seq) for seq in brown_tagged_words_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))\n",
    "\n",
    "MAX_SEQ_LENGTH = 180  \n",
    "brown_tagged_words_padded = pad_sequences(brown_tagged_words_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "brown_tagged_tags_padded = pad_sequences(brown_tagged_tags_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwQNJAtXMl2k"
   },
   "source": [
    "Implementing word2vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwqw-pe6MSyK",
    "outputId": "76bab012-2f41-4f04-b505-d7d41a30a369"
   },
   "outputs": [],
   "source": [
    "path = \"GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec= KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "EMBEDDING_SIZE  = 300 \n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "word2id = word_tokenizer.word_index\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57340, 180)\n"
     ]
    }
   ],
   "source": [
    "print(brown_tagged_tags_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_goCjUyCmhA"
   },
   "source": [
    "One-hot representation for tags since it has less no. of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57340, 180, 13)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "brown_tagged_vect= to_categorical(brown_tagged_tags_padded)\n",
    "print(brown_tagged_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n",
      "(36697, 180) (36697, 180, 13)\n",
      "Epoch 1/3\n",
      "287/287 [==============================] - 15s 44ms/step - loss: 0.8773 - accuracy: 0.9563 - val_loss: 0.1927 - val_accuracy: 0.9661\n",
      "Epoch 2/3\n",
      "287/287 [==============================] - 12s 42ms/step - loss: 0.1902 - accuracy: 0.9661 - val_loss: 0.1862 - val_accuracy: 0.9669\n",
      "Epoch 3/3\n",
      "287/287 [==============================] - 14s 48ms/step - loss: 0.1873 - accuracy: 0.9665 - val_loss: 0.1846 - val_accuracy: 0.9671\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 0.2079 - accuracy: 0.9628\n",
      "359/359 [==============================] - 3s 7ms/step\n",
      "(36697, 180) (36697, 180, 13)\n",
      "Epoch 1/3\n",
      "287/287 [==============================] - 16s 51ms/step - loss: 0.8633 - accuracy: 0.9567 - val_loss: 0.1956 - val_accuracy: 0.9657\n",
      "Epoch 2/3\n",
      "287/287 [==============================] - 15s 51ms/step - loss: 0.1919 - accuracy: 0.9658 - val_loss: 0.1890 - val_accuracy: 0.9664\n",
      "Epoch 3/3\n",
      "287/287 [==============================] - 15s 52ms/step - loss: 0.1891 - accuracy: 0.9663 - val_loss: 0.1875 - val_accuracy: 0.9667\n",
      "359/359 [==============================] - 3s 9ms/step - loss: 0.2002 - accuracy: 0.9639\n",
      "359/359 [==============================] - 3s 7ms/step\n",
      "(36697, 180) (36697, 180, 13)\n",
      "Epoch 1/3\n",
      "287/287 [==============================] - 16s 52ms/step - loss: 0.9180 - accuracy: 0.9574 - val_loss: 0.1937 - val_accuracy: 0.9662\n",
      "Epoch 2/3\n",
      "287/287 [==============================] - 15s 51ms/step - loss: 0.1875 - accuracy: 0.9666 - val_loss: 0.1859 - val_accuracy: 0.9669\n",
      "Epoch 3/3\n",
      "287/287 [==============================] - 15s 51ms/step - loss: 0.1845 - accuracy: 0.9670 - val_loss: 0.1844 - val_accuracy: 0.9671\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 0.2187 - accuracy: 0.9607\n",
      "359/359 [==============================] - 2s 7ms/step\n",
      "(36697, 180) (36697, 180, 13)\n",
      "Epoch 1/3\n",
      "287/287 [==============================] - 16s 54ms/step - loss: 0.9629 - accuracy: 0.9545 - val_loss: 0.2085 - val_accuracy: 0.9638\n",
      "Epoch 2/3\n",
      "287/287 [==============================] - 15s 52ms/step - loss: 0.1992 - accuracy: 0.9645 - val_loss: 0.1985 - val_accuracy: 0.9645\n",
      "Epoch 3/3\n",
      "287/287 [==============================] - 15s 52ms/step - loss: 0.1957 - accuracy: 0.9649 - val_loss: 0.1969 - val_accuracy: 0.9647\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 0.1706 - accuracy: 0.9698\n",
      "359/359 [==============================] - 3s 7ms/step\n",
      "(36697, 180) (36697, 180, 13)\n",
      "Epoch 1/3\n",
      "287/287 [==============================] - 15s 51ms/step - loss: 0.9031 - accuracy: 0.9539 - val_loss: 0.2090 - val_accuracy: 0.9633\n",
      "Epoch 2/3\n",
      "287/287 [==============================] - 15s 52ms/step - loss: 0.2020 - accuracy: 0.9639 - val_loss: 0.2006 - val_accuracy: 0.9641\n",
      "Epoch 3/3\n",
      "287/287 [==============================] - 15s 52ms/step - loss: 0.1987 - accuracy: 0.9645 - val_loss: 0.1989 - val_accuracy: 0.9644\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 0.1603 - accuracy: 0.9716\n",
      "359/359 [==============================] - 3s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(len(brown_tagged_words))\n",
    "len_brown_data=len(brown_tagged_words_padded)/5\n",
    "avg_loss=0.0\n",
    "avg_acc=0.0\n",
    "confusion_mat=[]\n",
    "per_pos_acc=[]\n",
    "Y_pred=[]\n",
    "Y_true=[]\n",
    "for i in range(5):\n",
    "    first_part=int(i*(len_brown_data))\n",
    "    second_part=int((i+1)*len_brown_data)\n",
    "    X_train=np.concatenate((brown_tagged_words_padded[:first_part],brown_tagged_words_padded[second_part:]),axis=0)\n",
    "    X_test =brown_tagged_words_padded[first_part:second_part]\n",
    "    Y_train=np.concatenate((brown_tagged_vect[:first_part],brown_tagged_vect[second_part:]),axis=0)\n",
    "    Y_test =brown_tagged_vect[first_part:second_part]\n",
    "    Y_test_enc=brown_tagged_tags_encoded[first_part:second_part]\n",
    "    X_train,X_validation=train_test_split(X_train,train_size=0.80,test_size=0.20,random_state = 50)\n",
    "    Y_train,Y_validation=train_test_split(Y_train,train_size=0.80,test_size=0.20,random_state = 50)\n",
    "    print(X_train.shape,Y_train.shape)\n",
    "    network = models.Sequential() ## starting the ffnn\n",
    "    network.add(Embedding(input_dim     = VOCABULARY_SIZE,\n",
    "                             output_dim    = EMBEDDING_SIZE,\n",
    "                             input_length  = MAX_SEQ_LENGTH,\n",
    "                             weights       = [embedding_weights],\n",
    "                             trainable     = False))\n",
    "    network.add(layers.Dense(units=100, activation='ReLU'))\n",
    "    network.add(layers.Dense(units=13, activation='softmax'))\n",
    "    network.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "                optimizer='rmsprop', # Root Mean Square Propagation\n",
    "                metrics=['accuracy'])\n",
    "    trainng = network.fit(X_train,Y_train,batch_size=128,epochs=3,validation_data=(X_validation, Y_validation))\n",
    "    loss,accuracy = network.evaluate(X_test,Y_test,verbose=1)\n",
    "    avg_loss=avg_loss+loss\n",
    "    avg_acc=avg_acc+accuracy\n",
    "    pred=network.predict(X_test)\n",
    "    indices=np.argmax(pred, axis=2)\n",
    "    #not_zero=indices[indices != 0]\n",
    "    Y_pred=indices \n",
    "    Y_true=brown_tagged_tags_padded[first_part:second_part]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as re_lu_4_layer_call_fn, re_lu_4_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 180, 300)          14944800  \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 180, 100)          30100     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 180, 13)           1313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,976,213\n",
      "Trainable params: 31,413\n",
      "Non-trainable params: 14,944,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.save(\"my_model_1\")\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is a boy\n"
     ]
    }
   ],
   "source": [
    "sentence=input()\n",
    "\n",
    "from gensim import models\n",
    "import tensorflow as tf\n",
    "def sen_vec(sent):\n",
    "    words=sent.split(\" \")\n",
    "    l = []\n",
    "    for i in range(len(words)):\n",
    "        x=0\n",
    "        for j in range(len(brown_tagged_words_encoded)):\n",
    "            for k in range(len(brown_tagged_words_encoded[j])):\n",
    "                if(x==0 and words[i]==brown_tagged_words[j][k]):\n",
    "                    l.append(int(brown_tagged_words_encoded[j][k]))\n",
    "                    x=1\n",
    "                    break\n",
    "            if(x==1):\n",
    "                break\n",
    "        if(x==0):\n",
    "            sim_word = Word2Vec.most_similar(positive=words[i],topn=10)[:][0]\n",
    "            while(x==0):\n",
    "                for j in range(len(brown_tagged_words_encoded)):\n",
    "                    for k in range(len(brown_tagged_words_encoded[j])):\n",
    "                        if(x==0 and words[i]==brown_tagged_words[j][k]):\n",
    "                            l.append(int(brown_tagged_words_encoded[j][k]))\n",
    "                            x=1\n",
    "                            break\n",
    "                    if(x==1):\n",
    "                        break\n",
    "                \n",
    "                            \n",
    "                        \n",
    "                \n",
    "    t=tf.convert_to_tensor(l)\n",
    "    t=tf.reshape(t, [1, t.shape[0]])\n",
    "    y = pad_sequences(t, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "(1, 180, 13) 176 180\n",
      "PRON\n",
      "VERB\n",
      "***\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "l=[]\n",
    "l=sen_vec(sentence)\n",
    "#print(l.shape,l)\n",
    "dict={5:\"DET\",1:\"NOUN\",7:\"ADV\" ,2:\"VERB\",4:\"ADP\",6:\"ADJ\",3:\".\",9:\"CONJ\",8:\"PRON\" ,10:\"PRT\",11:\"NUM\",12:\"X\"}\n",
    "model1 = keras.models.load_model('my_model_1')\n",
    "pred=model1.predict(l)\n",
    "ans=[]\n",
    "x=sentence.split(\" \")\n",
    "start=MAX_SEQ_LENGTH-len(x)\n",
    "print(pred.shape , start, MAX_SEQ_LENGTH)\n",
    "for i in range(start,MAX_SEQ_LENGTH):\n",
    "    indices = tf.argmax(pred[0][i])\n",
    "    c = tf.keras.backend.eval(indices)\n",
    "    #print(c)\n",
    "    if c==0:\n",
    "        print(\"***\")\n",
    "    else:\n",
    "        print(dict[c])\n",
    "    #ans.append(dict[c])\n",
    "#print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11468, 180) (11468, 180)\n",
      "(13, 13)\n",
      "[[28781  1050     0    35     0   588   156    18     0     2   166     7]\n",
      " [ 1264 30368     0    64     1   262   153     0     0    53     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [   10     6     0 12871     0    53   201     0    22   315     0     0]\n",
      " [    3     0     0   569 15733     0     0     6    17     0     2     0]\n",
      " [  353   176     0    11     1  8337   498     0     0     3     0     0]\n",
      " [   97    48     0   526    93   536  8618     0    19   307     0     0]\n",
      " [   15     4     0   232   570     3     1 12939     0     0     0     0]\n",
      " [    0     0     0     3    12     0     0     0  1379     0     0     0]\n",
      " [  123    37     0   420     2    38   101     1     0  3021     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0   986     0]\n",
      " [   75    10     0     2     0     5     3     0     0     0     0    12]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_true.shape,Y_pred.shape)\n",
    "Y_true=np.reshape(Y_true, (2064240,1))\n",
    "Y_pred=np.reshape(Y_pred, (2064240,1))\n",
    "cm=confusion_matrix(Y_true, Y_pred)\n",
    "print(cm.shape)\n",
    "\n",
    "print(cm[1:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>VERB</th>\n",
       "      <th>.</th>\n",
       "      <th>ADP</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>PRT</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>28781</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>588</td>\n",
       "      <td>156</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>166</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>1264</td>\n",
       "      <td>30368</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>262</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>12871</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>15733</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>353</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8337</td>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>97</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>526</td>\n",
       "      <td>93</td>\n",
       "      <td>536</td>\n",
       "      <td>8618</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>570</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>123</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NOUN   VERB  .    ADP    DET   ADJ   ADV   PRON  CONJ   PRT  NUM   X\n",
       "NOUN  28781   1050  0     35      0   588   156     18     0     2  166   7\n",
       "VERB   1264  30368  0     64      1   262   153      0     0    53    0   0\n",
       ".         0      0  0      0      0     0     0      0     0     0    0   0\n",
       "ADP      10      6  0  12871      0    53   201      0    22   315    0   0\n",
       "DET       3      0  0    569  15733     0     0      6    17     0    2   0\n",
       "ADJ     353    176  0     11      1  8337   498      0     0     3    0   0\n",
       "ADV      97     48  0    526     93   536  8618      0    19   307    0   0\n",
       "PRON     15      4  0    232    570     3     1  12939     0     0    0   0\n",
       "CONJ      0      0  0      3     12     0     0      0  1379     0    0   0\n",
       "PRT     123     37  0    420      2    38   101      1     0  3021    0   0\n",
       "NUM       0      0  0      0      0     0     0      0     0     0  986   0\n",
       "X        75     10  0      2      0     5     3      0     0     0    0  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagset=[\"NOUN\",\"VERB\",\".\",\"ADP\",\"DET\",\"ADJ\",\"ADV\" ,\"PRON\" ,\"CONJ\",\"PRT\",\"NUM\",\"X\"]\n",
    "conf_df = pd.DataFrame(cm[1:,1:], columns = list(tagset),index=list(tagset))\n",
    "display(conf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97455647 0.93685101 0.95801129 0.         0.87361705 0.95862783\n",
      " 0.8488088  0.88562327 0.99807158 0.95963814 0.81626587 0.85441941\n",
      " 0.63157895]\n",
      "[1.         0.84575375 0.9404478  0.         0.69822068 0.78547179\n",
      " 0.84596651 0.83678027 0.93869704 0.23083361 0.46384155 0.91977612\n",
      " 0.06629834]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "prec_score=precision_score(Y_true, Y_pred, average=None)\n",
    "print(prec_score)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "rec_score=recall_score(Y_true, Y_pred, average=None)\n",
    "print(rec_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9746    1.0000    0.9871   1882694\n",
      "           1     0.9369    0.8458    0.8890     34030\n",
      "           2     0.9580    0.9404    0.9491     32291\n",
      "           3     0.0000    0.0000    0.0000     29083\n",
      "           4     0.8736    0.6982    0.7761     18434\n",
      "           5     0.9586    0.7855    0.8635     20030\n",
      "           6     0.8488    0.8460    0.8474      9855\n",
      "           7     0.8856    0.8368    0.8605     10299\n",
      "           8     0.9981    0.9387    0.9675     13784\n",
      "           9     0.9596    0.2308    0.3721      5974\n",
      "          10     0.8163    0.4638    0.5915      6513\n",
      "          11     0.8544    0.9198    0.8859      1072\n",
      "          12     0.6316    0.0663    0.1200       181\n",
      "\n",
      "    accuracy                         0.9717   2064240\n",
      "   macro avg     0.8228    0.6594    0.7008   2064240\n",
      "weighted avg     0.9574    0.9717    0.9633   2064240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(Y_true),np.array(Y_pred),digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9871143081172807, 1: 0.8889746876496116, 2: 0.949148304422566, 3: nan, 4: 0.7761329031869026, 5: 0.8634542560781515, 6: 0.847385272145144, 7: 0.8605092361457813, 8: 0.9674742036787797, 9: 0.3721495074888679, 10: 0.5915410221264931, 11: 0.8858939802336029, 12: 0.11999999999999998}\n",
      "{0: 0.9948055628973067, 1: 0.862527795925462, 2: 0.9439087919534014, 3: nan, 4: 0.7274299472131481, 5: 0.8149111175568723, 6: 0.84653344705739, 7: 0.8461130637972001, 8: 0.9500000000000001, 9: 0.2721746338767615, 10: 0.5076798978254294, 11: 0.9059169423006246, 12: 0.08075370121130553}\n",
      "{0: 0.9795410694726642, 1: 0.9170947143021017, 2: 0.9544463092521702, 3: nan, 4: 0.8318253496367913, 5: 0.9181470155699245, 6: 0.8482388132592638, 7: 0.8754037746581883, 8: 0.9856032906764169, 9: 0.5882102030370243, 10: 0.7085893887507623, 11: 0.8667369901547116, 12: 0.23346303501945526}\n",
      "0.9573675130805994 0.9716597876215944\n",
      "0.9644607042078566 0.9687672958731742 0.9601922324263614\n",
      "[0.97455647 0.93685101 0.95801129 0.         0.87361705 0.95862783\n",
      " 0.8488088  0.88562327 0.99807158 0.95963814 0.81626587 0.85441941\n",
      " 0.63157895]\n",
      "[1.         0.84575375 0.9404478  0.         0.69822068 0.78547179\n",
      " 0.84596651 0.83678027 0.93869704 0.23083361 0.46384155 0.91977612\n",
      " 0.06629834]\n"
     ]
    }
   ],
   "source": [
    "def FValues(precision , recall , n):\n",
    "  # return (1+n*n)*prec*recall\n",
    "  fval={}\n",
    "  for i in range(13):\n",
    "    fval[i]=(1+n*n)*(precision[i]*recall[i])/((n*n)*(precision[i])+recall[i])\n",
    "    # fval[i]=2*precision[i]*recall[i]/(precision[i]+recall[i])\n",
    "  return fval\n",
    "\n",
    "f1=FValues(prec_score,rec_score,1)\n",
    "f2=FValues(prec_score,rec_score,2)\n",
    "f05=FValues(prec_score,rec_score,0.5)\n",
    "\n",
    "\n",
    "\n",
    "print(f1)\n",
    "print(f2)\n",
    "print(f05)\n",
    "\n",
    "prec_score_overall=precision_score(Y_true, Y_pred, average='weighted')\n",
    "rec_score_overall=recall_score(Y_true, Y_pred, average='weighted')\n",
    "print(prec_score_overall,rec_score_overall)\n",
    "f_1=(1+1*1)*(prec_score_overall*rec_score_overall)/((1*1)*(prec_score_overall)+rec_score_overall)\n",
    "f_2=(1+2*2)*(prec_score_overall*rec_score_overall)/((2*2)*(prec_score_overall)+rec_score_overall)\n",
    "f_5=(1+0.5*0.5)*(prec_score_overall*rec_score_overall)/((0.5*0.5)*(prec_score_overall)+rec_score_overall)\n",
    "print(f_1,f_2,f_5)\n",
    "print(prec_score)\n",
    "print(rec_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a62091775dced0a567347656cd306393505080d999f2a7c4d682e585f94130a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
